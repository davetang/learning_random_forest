---
title: "Random Forests"
output:
   md_document:
      variant: markdown
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.path = "img/")
```

## Getting started

Install and load packages used in this notebook.

```{r load_packages}
my_packages <- c('randomForest', 'ggplot2')

for (my_package in my_packages){
   if(!require(my_package, character.only = TRUE)){
      install.packages(my_package, '/packages')
   }
  library(my_package, character.only = TRUE)
}
```

## Preparing the data

We will use the [Wine data set](https://archive.ics.uci.edu/ml/datasets/wine).

```{r data}
my_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'

wine_df <- read.csv(
  file = url(my_url),
  header=FALSE
)

colnames(wine_df) <- c(
  'class',
  'alcohol',
  'malic_acid',
  'ash',
  'ash_alcalinity',
  'magnesium',
  'total_phenols',
  'flavanoids',
  'nonflavanoid_phenols',
  'proanthocyanins',
  'colour',
  'hue',
  'od280_od315',
  'proline'
)

wine_df$class <- as.factor(wine_df$class)

table(wine_df$class)
```

## Training

First we'll split the data into a training (80%) and testing set (20%).

```{r split}
set.seed(1984)
idx <- sample(x = 1:nrow(wine_df), size = nrow(wine_df)*0.8)

train <- wine_df[idx, ]
test <- wine_df[-idx, ]

dim(train)
dim(test)
```

Train a random forests model, where:

* `data` - an optional data frame containing the variables in the model
* `importance` - calculate the importance of predictors
* `do.trace` - give a more verbose output as randomForest is running
* `proximity` - calculate the proximity measure among the rows

```{r train_rf}
rf <- randomForest(
  class ~ .,
  data = train,
  importance = TRUE,
  do.trace = 100,
  proximity = TRUE
)
```

Summary.

```{r}
rf
```

## Plots

Variable importance plot.

```{r var_imp_plot}
varImpPlot(rf)
```

We can see a nice separation in the classes by plotting proline versus colour.

```{r plots}
ggplot(wine_df, aes(x = proline, y = colour, colour = class)) +
  geom_point() +
  theme_bw()
```

## Inspecting the Random Forest object

The model is saved as a `randomForest` class.

```{r rf_class}
class(rf)
```

The object is a list containing the model's settings, predictions, and other information.

```{r object}
str(rf)
```

`call` contains the original call to `randomForest`.

```{r call}
rf$call
```

`type` shows the model type: one of regression, classification, or unsupervised.

```{r type}
rf$type
```

`predicted` contains the predicted values of the input data based on out-of-bag samples.

```{r predicted}
rf$predicted
```

`importance` contains a matrix with number of classes + 2 (for classification) or two (for regression) columns. For our example, the first three columns are the class-specific measures computed as mean descrease in accuracy. The last two columns are the `MeanDecreaseAccuracy`, which is the mean decrease in accuracy over all classes and the `MeanDecreaseGini`, which is the mean decrease in Gini index.

```{r importance}
rf$importance
```

`importanceSD` contains the "standard errors" of the permutation-based importance measure.

```{r importance_sd}
rf$importanceSD
```

`ntree` shows the number of trees grown.

```{r ntree}
rf$ntree
```

`mtry` shows the number of predictors sampled for splitting at each node of an independent tree.

```{r myry}
rf$mtry
```

`forest` is a list that contains the entire forest, i.e. every grown tree.

```{r forest}
names(rf$forest)
```

We can use `getTree` to obtain an individual tree. The returned value is a matrix (or data frame, if `labelVar=TRUE`) with six columns and number of rows equal to total number of nodes in the tree. The six columns are:

1. left daughter - the row where the left daughter node is; 0 if the node is terminal
2. right daughter - the row where the right daughter node is; 0 if the node is terminal
3. split var - which variable was used to split the node; 0 if the node is terminal
4. split point - where the best split is; see Details for categorical predictor
5. status - is the node terminal (-1) or not (1)
6. prediction - the prediction for the node; 0 if the node is not terminal

```{r get_tree}
getTree(rf, k = 500, labelVar = TRUE)
```

Summary depth of all trees.

```{r tree_depth_summary}
summary(
  sapply(1:rf$ntree, function(x){
    nrow(getTree(rf, k = x))
  })
)
```

`err.rate` contains error rates of the prediction on the input data, the i-th element being the (OOB) error rate for all trees up to the i-th.

```{r err_rate}
head(rf$err.rate)
```

`confusion` contains the confusion matrix of the prediction (based on OOB data).

```{r confusion}
rf$confusion
```

`votes` is a matrix with one row for each input data point and one column for each class, giving the fraction or number of (OOB) "votes" from the random forest.

```{r votes}
head(rf$votes)
```

`oob.times` contains the number of times cases are "out-of-bag" (and thus used in computing OOB error estimate): around 36% of the time.

```{r oob_times}
summary(rf$oob.times / rf$ntree)
```

`proximity` contains a matrix of proximity measures among the input (based on the frequency that pairs of data points are in the same terminal nodes).

```{r proximity}
dim(rf$proximity)
```

## On importance

Notes from [Stack Exchange](http://stats.stackexchange.com/questions/92419/relative-importance-of-a-set-of-predictors-in-a-random-forests-classification-in>):

MeanDecreaseGini is a measure of variable importance based on the Gini impurity index used for the calculation of splits during training. A common misconception is that the variable importance metric refers to the Gini used for asserting model performance which is closely related to AUC, but this is wrong. Here is the explanation from the randomForest package written by Breiman and Cutler:

> Every time a split of a node is made on variable m the gini impurity criterion for the two descendent nodes is less than the parent node. Adding up the gini decreases for each individual variable over all trees in the forest gives a fast variable importance that is often very consistent with the permutation importance measure.

The Gini impurity index is defined as:

$$ G = \sum^{n_c}_{i=1} p_i (1 - p_i) $$

where $n_c$ is the number of classes in the target variable and $p_i$ is the ratio of this class.

## Session information

```{r session_info, echo=FALSE}
sessionInfo()
```
